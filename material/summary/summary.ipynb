{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공신경망 (이진) 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 1) 데이터 로드\n",
    "sales_df = pd.read_csv('../../data/sales_scaled.csv')\n",
    "X_train, X_test, y_train, y_test=train_test_split(\n",
    "  sales_df.drop('flag',axis=1), ## 목표변수 제외한 나머지\n",
    "  sales_df['flag'], ## 목표변수\n",
    "  test_size=0.2, ## 테스트셋 20%\n",
    "  random_state=123\n",
    ")\n",
    "\n",
    "# 2) 모델 성능 지표 출력 함수 정의\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score , recall_score, accuracy_score\n",
    "\n",
    "def get_clf_eval(y_test,pred):\n",
    "    confusion = confusion_matrix(y_test,pred)\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    recall = recall_score(y_test,pred)\n",
    "    precision = precision_score(y_test,pred)\n",
    "    print('confusion matrix')\n",
    "    print(confusion)\n",
    "    print(\"정확도: {0:.4f}. 재현율: {1:.4f}, 정밀도: {2:.4f}\".format(accuracy, recall, precision))\n",
    "\n",
    "# 3) Sklearn 기반의 MLP(다층 퍼셉트론)\n",
    "## 3-1) 모델 생성 및 학습\n",
    "mlp_clf = MLPClassifier(\n",
    "  hidden_layer_sizes=(10,10), ## 은닉층 2개, 각각 10개의 노드\n",
    "  max_iter=300 ## 최대 300번 반복\n",
    ")\n",
    "mlp_clf.fit(X_train , y_train)\n",
    "\n",
    "## 3-2) 예측 및 성능 지표 출력\n",
    "mlp_pred = mlp_clf.predict(X_test)\n",
    "get_clf_eval(y_test,mlp_pred)\n",
    "\n",
    "\n",
    "# 4) Keras 기반의 MLP(다층 퍼셉트론)\n",
    "from keras import models\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "## 4-1) 모델 정의\n",
    "model=models.Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],))) ## 입력층\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "## 4-2) 모델 컴파일 및 학습\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history=model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=200,batch_size=128,\n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "## 4-3) 학습 결과 확인 및 평가 & 시각화\n",
    "history_dict=history.history\n",
    "print(history_dict.keys())\n",
    "  ## dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "\n",
    "acc=history.history['accuracy'] ## 학습셋 정확도\n",
    "val_acc=history.history['val_accuracy'] ## 검증셋 정확도\n",
    "loss=history.history['loss'] ## 학습셋 손실\n",
    "val_loss=history.history['val_loss'] ## 검증셋 손실\n",
    "epochs=range(1,len(acc)+1) ## x축\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "### Loss 그래프\n",
    "plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### Accuracy 그래프\n",
    "plt.plot(epochs,acc,'bo',label='Training accuracy')\n",
    "plt.plot(epochs,val_acc,'b',label='Validation accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### 4-4) 성능 지표 출력\n",
    "predicted_result=model.predict(X_test)\n",
    "predicted_target=pd.Series([\n",
    "    1 if predicted_result[i]> 0.5 else 0 for i in range(0,predicted_result.shape[0])\n",
    "    # 0.5를 기준으로 1과 0으로 변환\n",
    "])\n",
    "get_clf_eval(y_test,predicted_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공신경망 손글씨 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.shape ## (60000, 28, 28)\n",
    "\n",
    "# 2) 데이터 전처리\n",
    "## 3차원 데이터를 2차원으로 변환\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "## 정규화 (0~1 사이 값으로 변환)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "## 목표변수 One-Hot 인코딩\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# 3) 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(784,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# 4) 모델 컴파일 및 학습\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    "  optimizer='adam'\n",
    ")\n",
    "history = model.fit(\n",
    "  x_train, y_train,\n",
    "  batch_size=batch_size,\n",
    "  epochs=epochs,\n",
    "  verbose=1, # 학습 과정 출력 여부\n",
    "  validation_split=0.3\n",
    ")\n",
    "\n",
    "# 5) 학습 결과 확인 및 평가 & 시각화\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epoch_range = range(1, len(acc) + 1) ## x축\n",
    "\n",
    "## Loss 그래프\n",
    "plt.plot(epoch_range, loss, 'bo', label='Training loss')\n",
    "plt.plot(epoch_range, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "## Accuracy 그래프\n",
    "plt.plot(epoch_range, acc, 'bo', label='Training acc')\n",
    "plt.plot(epoch_range, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (딥러닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드 및 전처리\n",
    "\n",
    "## 데이터셋 로드 (Training, Test 데이터셋 분할)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)  # (60000, 28, 28) 이미지수, 높이, 너비\n",
    "\n",
    "## CNN 입력을 위한 Reshape\n",
    "  ### CNN 모델은 4차원 입력을 받음 (이미지수, 높이, 너비, 채널수)\n",
    "  ### 흑백 이미지의 채널수는 1, 컬러 이미지의 채널수는 3\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "## 독립변수 정규화 (0~255 -> 0~1)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "## 목표변수 One-Hot Encoding 처리\n",
    "Y_train = to_categorical(y_train, 10)\n",
    "Y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 2) 모델 생성\n",
    "## CNN 모델 생성\n",
    "model = Sequential([\n",
    "  Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # 첫 번째 Conv 레이어, 입력 형태 지정\n",
    "  Conv2D(32, (3, 3), activation='relu'),  # 두 번째 Conv 레이어\n",
    "  MaxPooling2D(pool_size=(2, 2)),  # MaxPooling 레이어로 차원 축소\n",
    "  Dropout(0.25),  # Dropout 레이어로 과적합 방지\n",
    "  Flatten(),  # 1차원으로 변환\n",
    "  Dense(128, activation='relu'),  # Fully Connected 레이어\n",
    "  Dropout(0.5),  # Dropout 레이어로 과적합 방지\n",
    "  Dense(10, activation='softmax')  # 출력 레이어, 10개의 클래스\n",
    "])\n",
    "\n",
    "## 모델 요약 출력\n",
    "print(model.summary())\n",
    "\n",
    "# 3) 모델 컴파일 및 학습\n",
    "## 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "## 모델 학습\n",
    "history = model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# 4) 모델 평가\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(model.metrics_names)\n",
    "print(score)\n",
    "\n",
    "# 5) 성능 지표 시각화\n",
    "\n",
    "## 성능 지표 시각화\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "epoch_range = range(1, len(acc) + 1)\n",
    "\n",
    "## Loss 시각화\n",
    "plt.plot(epoch_range, loss, 'b', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "## Accuracy 시각화\n",
    "plt.plot(epoch_range, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print(X_train.shape)\n",
    "  ## (50000, 32, 32, 3) 이미지수, 높이, 너비, 채널수\n",
    "\n",
    "# 2) 데이터 전처리\n",
    "## CNN 모델은 4차원 입력을 받음 (이미지수, 높이, 너비, 채널수)\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "\n",
    "## 독립변수 정규화 (0~255 -> 0~1) \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "## 목표변수 One-Hot Encoding 처리\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, 10) ## 10개의 클래스\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 3) 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(\n",
    "  32, # 필터 수\n",
    "  (3,3), # 필터 크기\n",
    "  activation='relu', # 활성화 함수\n",
    "  input_shape=(32, 32, 3) # 입력 형태\n",
    "))\n",
    "print(model.output_shape) # (None, 30, 30, 32)\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # MaxPooling 레이어로 차원 축소\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())  # 1차원으로 변환\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout 레이어로 과적합 방지\n",
    "model.add(Dense(10, activation='softmax')) # 출력 레이어, 10개의 클래스\n",
    "print(model.summary()) # Layer, Output Shape, Param 등 출력\n",
    "\n",
    "# 4) 모델 컴파일 및 학습\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "history=model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# 5) 성능 평가\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(model.metrics_names)\n",
    "print(score)\n",
    "\n",
    "# 6) 성능 지표 시각화\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "epoch_range = range(1, len(acc) + 1) ## epoch 범위\n",
    "plt.plot(epoch_range, loss, 'b', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epoch_range, acc, 'b', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드\n",
    "customer_df = pd.read_csv('salesdata.csv')\n",
    "\n",
    "# 2) 데이터 정보 확인\n",
    "## 데이터 정보 출력\n",
    "customer_df.info()\n",
    "\n",
    "## 각 컬럼의 고유값 확인\n",
    "for col in customer_df.columns:\n",
    "    print(col, customer_df[col].unique())\n",
    "    ## flag ['Y' 'N']\n",
    "\n",
    "# 3) 결측값 확인\n",
    "## 결측값이 있는 컬럼 확인 및 결측값 수 확인\n",
    "customer_df.isnull().sum() ## 칼럼명 결측치 수를 시리즈로 출력\n",
    "\n",
    "## 결측값 비율 확인\n",
    "(customer_df.isnull().sum() / customer_df.shape[0]) * 100\n",
    "\n",
    "# 4) 기초 시각화\n",
    "import seaborn as sns\n",
    "## countplot: \n",
    "sns.countplot(x='BikeBuyer', data=customer_df)\n",
    "## histplot\n",
    "sns.histplot(x='AvgMonthSpend', data=customer_df)\n",
    "\n",
    "# 5) 결측값 처리\n",
    "## 결측값이 있는 행 제거\n",
    "customer_df.dropna(subset=['column명'], inplace=True) ## 특정 칼럼에 결측값이 있을 경우 해당 행을 제거\n",
    "\n",
    "## 결측행 채우기\n",
    "customer_df.fillna('값', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 레이블 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**레이블 인코딩**\n",
    "\n",
    "레이블 인코딩이란?\n",
    "레이블 인코딩은 범주형 데이터 (보통 서열 척도)를 숫자로 변환하는 기법입니다. \n",
    "\n",
    "예를 들어, '사과', '바나나', '체리'라는 범주형 데이터가 있을 때, \n",
    "이를 각각 0, 1, 2로 변환할 수 있습니다. (영어 > 한글 사전순)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1) 데이터 로드\n",
    "items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
    "\n",
    "# 2) 인코딩 변환\n",
    "encoder = LabelEncoder() ## 인코더 초기화\n",
    "encoder.fit(items) ## 인코더 학습\n",
    "labels = encoder.transform(items) ## 문자열 -> 인코딩된 숫자로 변환\n",
    "print('인코딩 변환값:',labels) # [0 1 4 5 3 3 2 2]\n",
    "\n",
    "# 3) 인코딩 클래스 (고유값) 확인\n",
    "print('인코딩 클래스:',encoder.classes_) # ['TV' '냉장고' '믹서' '선풍기' '전자렌지' '컴퓨터']\n",
    "\n",
    "# 4) 디코딩\n",
    "print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동 인코딩 (df.replace)\n",
    "sales_df = pd.read_csv('sales_na.csv')\n",
    "sales_df['flag'].replace([\"N\",\"Y\"],[0,1],inplace=True)\n",
    "\n",
    "# 데이터프레임의 각 칼럼을 라벨 인코딩\n",
    "le = LabelEncoder()\n",
    "for col in sales_df.columns:\n",
    "  le.fit(customer_df[col])\n",
    "  customer_df[col] = le.transform(customer_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원 핫 인코딩과 Min-Max 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원 핫 인코딩(One-Hot Encoding)\n",
    "\n",
    "원 핫 인코딩(One-Hot Encoding)은 범주형 데이터 (주로 명목 척도)를 수치형 데이터로 변환하는 기법 중 하나입니다.\n",
    "이 방법은 각 범주를 고유한 이진 벡터로 변환합니다.\n",
    "예를 들어, '빨강', '초록', '파랑'이라는 세 가지 범주가 있을 때,\n",
    "이를 원 핫 인코딩하면 다음과 같이 변환됩니다:\n",
    "\n",
    "빨강: [1, 0, 0]\n",
    "초록: [0, 1, 0]\n",
    "파랑: [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['item'], dtype='object')\n",
      "Index(['item_TV', 'item_냉장고', 'item_믹서', 'item_선풍기', 'item_전자렌지', 'item_컴퓨터'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) 데이터 로드\n",
    "df = pd.DataFrame({'item':['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] })\n",
    "print(df.columns)\n",
    "\n",
    "# 2) 원핫 인코딩 - 특정 칼럼을 원핫 인코딩한 칼럼들로 대체\n",
    "one_hot_result = pd.get_dummies(df, columns=['item'])\n",
    "  ## 해당 칼럼을 원핫인코딩으로 변환 (기존 칼럼은 제거)\n",
    "print(one_hot_result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_df 데이터셋\n",
    "\n",
    "# 1) 데이터 로드\n",
    "customer_df = pd.read_csv('../../data/customer_na.csv')\n",
    "\n",
    "# 2) 원핫 인코딩\n",
    "## replace 인코딩\n",
    "customer_df['Gender'] = customer_df['Gender'].replace(['M', 'F'], [0, 1])\n",
    "customer_df['MaritalStatus'] = customer_df['MaritalStatus'].replace(['S', 'M'], [0, 1])\n",
    "\n",
    "## get_dummies 인코딩\n",
    "columns = ['CountryRegionName', 'Education', 'Occupation']\n",
    "customer_df = pd.get_dummies(customer_df, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max 스케일링 (Min-Max Scaling)\n",
    "\n",
    "Min-Max 스케일링은 데이터의 값을 0과 1 사이의 값으로 변환하는 정규화 기법입니다.\n",
    "```\n",
    "수식: X_scaled = (X - X_min) / (X_max - X_min)\n",
    "```\n",
    "\n",
    "\n",
    "이 외에 정규화, Feature Scaling 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Gender  MaritalStatus  HomeOwnerFlag  NumberCarsOwned  \\\n",
      "0         0.0            1.0            1.0              0.6   \n",
      "1         0.0            1.0            1.0              0.4   \n",
      "2         1.0            0.0            0.0              0.6   \n",
      "3         0.0            1.0            1.0              0.4   \n",
      "4         0.0            0.0            1.0              0.2   \n",
      "...       ...            ...            ...              ...   \n",
      "18350     1.0            1.0            1.0              0.2   \n",
      "18351     1.0            0.0            0.0              0.2   \n",
      "18352     1.0            0.0            0.0              0.0   \n",
      "18353     1.0            0.0            0.0              0.2   \n",
      "18354     1.0            1.0            1.0              0.0   \n",
      "\n",
      "       NumberChildrenAtHome  TotalChildren  YearlyIncome  BikeBuyer  \\\n",
      "0                  0.000000       0.333333      0.496842        1.0   \n",
      "1                  0.333333       0.666667      0.489453        1.0   \n",
      "2                  0.000000       0.000000      0.536172        1.0   \n",
      "3                  0.333333       0.666667      0.317083        1.0   \n",
      "4                  0.000000       0.000000      0.231958        1.0   \n",
      "...                     ...            ...           ...        ...   \n",
      "18350              0.000000       0.666667      0.544379        0.0   \n",
      "18351              0.000000       0.000000      0.518464        1.0   \n",
      "18352              0.000000       0.000000      0.224543        1.0   \n",
      "18353              0.000000       0.000000      0.072511        0.0   \n",
      "18354              0.333333       0.666667      0.038028        1.0   \n",
      "\n",
      "       AvgMonthSpend       Age  ...  Education_Bachelors  \\\n",
      "0           0.324210  0.185714  ...                  1.0   \n",
      "1           0.425201  0.400000  ...                  0.0   \n",
      "2           0.470977  0.214286  ...                  1.0   \n",
      "3           0.605474  0.328571  ...                  0.0   \n",
      "4           0.533742  0.357143  ...                  0.0   \n",
      "...              ...       ...  ...                  ...   \n",
      "18350       0.190656  0.571429  ...                  1.0   \n",
      "18351       0.289287  0.200000  ...                  0.0   \n",
      "18352       0.257669  0.200000  ...                  0.0   \n",
      "18353       0.345446  0.242857  ...                  0.0   \n",
      "18354       0.385559  0.285714  ...                  0.0   \n",
      "\n",
      "       Education_Graduate Degree  Education_High School  \\\n",
      "0                            0.0                    0.0   \n",
      "1                            0.0                    0.0   \n",
      "2                            0.0                    0.0   \n",
      "3                            0.0                    0.0   \n",
      "4                            0.0                    0.0   \n",
      "...                          ...                    ...   \n",
      "18350                        0.0                    0.0   \n",
      "18351                        1.0                    0.0   \n",
      "18352                        1.0                    0.0   \n",
      "18353                        0.0                    0.0   \n",
      "18354                        0.0                    0.0   \n",
      "\n",
      "       Education_Partial College  Education_Partial High School  \\\n",
      "0                            0.0                            0.0   \n",
      "1                            1.0                            0.0   \n",
      "2                            0.0                            0.0   \n",
      "3                            1.0                            0.0   \n",
      "4                            1.0                            0.0   \n",
      "...                          ...                            ...   \n",
      "18350                        0.0                            0.0   \n",
      "18351                        0.0                            0.0   \n",
      "18352                        0.0                            0.0   \n",
      "18353                        0.0                            1.0   \n",
      "18354                        1.0                            0.0   \n",
      "\n",
      "       Occupation_Clerical  Occupation_Management  Occupation_Manual  \\\n",
      "0                      1.0                    0.0                0.0   \n",
      "1                      1.0                    0.0                0.0   \n",
      "2                      1.0                    0.0                0.0   \n",
      "3                      0.0                    0.0                0.0   \n",
      "4                      0.0                    0.0                0.0   \n",
      "...                    ...                    ...                ...   \n",
      "18350                  1.0                    0.0                0.0   \n",
      "18351                  1.0                    0.0                0.0   \n",
      "18352                  0.0                    0.0                0.0   \n",
      "18353                  0.0                    0.0                1.0   \n",
      "18354                  0.0                    0.0                1.0   \n",
      "\n",
      "       Occupation_Professional  Occupation_Skilled Manual  \n",
      "0                          0.0                        0.0  \n",
      "1                          0.0                        0.0  \n",
      "2                          0.0                        0.0  \n",
      "3                          0.0                        1.0  \n",
      "4                          0.0                        1.0  \n",
      "...                        ...                        ...  \n",
      "18350                      0.0                        0.0  \n",
      "18351                      0.0                        0.0  \n",
      "18352                      0.0                        1.0  \n",
      "18353                      0.0                        0.0  \n",
      "18354                      0.0                        0.0  \n",
      "\n",
      "[18355 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 0) Min-Max 스케일러 초기화\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 1) 전처리: int -> float\n",
    "customer_df = customer_df.astype('float')\n",
    "\n",
    "# 2) Min-Max 스케일러 훈련 - Min과 Max 계산\n",
    "scaler.fit(customer_df)\n",
    "\n",
    "# 3) 데이터 변환 (numpy.ndarray로 반환)\n",
    "customer_df_scaled_array = scaler.transform(customer_df)\n",
    "\n",
    "# 4) 변환된 데이터를 DataFrame으로 변환\n",
    "scaler_df_scaled = pd.DataFrame(customer_df_scaled_array, columns=customer_df.columns)\n",
    "\n",
    "# 5) 결과 출력\n",
    "print(scaler_df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means 군집화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means 군집화 및 시각화 (+PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "# 1) 데이터 로드\n",
    "iris = load_iris()\n",
    "irisDF = pd.DataFrame(data=iris.data, columns=[\n",
    "  'sepal_length','sepal_width','petal_length','petal_width'\n",
    "])\n",
    "\n",
    "# 2) KMeans 모델 학습\n",
    "kmeans = KMeans(\n",
    "  n_clusters=3, ## 군집 개수\n",
    "  init='k-means++', ## 초기 중심점 설정 방식\n",
    "  max_iter=300, ## 최대 반복 횟수\n",
    "  random_state=0 ## 시드값\n",
    ").fit(irisDF)\n",
    "\n",
    "# 3) 군집 결과 확인\n",
    "kmeans.labels_ # 각 데이터가 포인트가 속한 군집 출력\n",
    "  ## [1 1 0 1 2 1 0 ...] 군집 0, 1, 2 중 하나로 분류\n",
    "kmeans.cluster_centers_ # 각 군집 중심점 좌표 (4차원)\n",
    "  ## [[5.9016129  2.7483871  4.39354839 1.43387097], [...], [...]]\n",
    "\n",
    "# 4) 군집 결과를 데이터프레임에 추가\n",
    "irisDF['cluster']=kmeans.labels_ # 군집 결과를 데이터프레임에 추가\n",
    "irisDF['target'] = iris.target # 실제 품종 (정답)\n",
    "\n",
    "# 5) 차원 축소: 4차원 데이터를 2차원으로\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## 5-1) PCA 모델 생성\n",
    "pca = PCA(n_components=2) ## n_components: 축소할 차원 수\n",
    "\n",
    "## 5-2) PCA 모델 학습\n",
    "pca_transformed = pca.fit_transform(iris.data)\n",
    "  ## 150개의 데이터가 [x좌표, y좌표] 형태의 2차원 데이터로 변환\n",
    "  ## array([[2.68420713,  0.32660731], ...]),\n",
    "\n",
    "## 5-3) PCA 데이터를 데이터프레임에 추가\n",
    "irisDF['pca_x'] = pca_transformed[:,0]\n",
    "irisDF['pca_y'] = pca_transformed[:,1]\n",
    "\n",
    "# 6) 군집 시각화\n",
    "\n",
    "## 6-1) Scatter plot (cluster 값이 0, 1, 2 인 경우마다 별도의 Index로 추출)\n",
    "marker0_ind = irisDF[irisDF['cluster']==0].index\n",
    "marker1_ind = irisDF[irisDF['cluster']==1].index\n",
    "marker2_ind = irisDF[irisDF['cluster']==2].index\n",
    "\n",
    "plt.scatter(\n",
    "  x=irisDF.loc[marker0_ind,'pca_x'], ## cluster값 0, 1, 2에 해당하는 Index로, 각 cluster 레벨의 pca_x, pca_y 값 추출\n",
    "  y=irisDF.loc[marker0_ind,'pca_y'], \n",
    "  marker='o') ## o, s, ^ 로 marker 표시\n",
    "plt.scatter(x=irisDF.loc[marker1_ind,'pca_x'], y=irisDF.loc[marker1_ind,'pca_y'], marker='s') # 사각형\n",
    "plt.scatter(x=irisDF.loc[marker2_ind,'pca_x'], y=irisDF.loc[marker2_ind,'pca_y'], marker='^') # 삼각형\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('3 Clusters Visualization by 2 PCA Components')\n",
    "plt.show()\n",
    "\n",
    "## 6-2) boxplot\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(6,6))\n",
    "g=sns.boxplot(data=irisDF, y='sepal_length',x='cluster')\n",
    "g.set_title('Sepal Length per Cluster')\n",
    "g.set_xlabel('Cluster')\n",
    "g.set_ylabel('Sepal Length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드\n",
    "boston_df = pd.read_csv('boston_housing.csv')\n",
    "\n",
    "# 2) Min-Max 스케일링\n",
    "boston_scaled = MinMaxScaler().fit_transform(boston_df.drop('medv',axis=1))\n",
    "boston_scaled_df=pd.DataFrame(data=boston_scaled,columns=boston_df.columns[:-1])\n",
    "\n",
    "# 3) K-means 군집화\n",
    "kmeans = KMeans(n_clusters=7, init='k-means++', max_iter=300,random_state=0).fit(boston_scaled_df)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "boston_df['cluster']=kmeans.labels_\n",
    "\n",
    "# 4) PCA 차원축소 및 시각화\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_transformed = pca.fit_transform(boston_scaled_df)\n",
    "boston_df['pca_x'] = pca_transformed[:,0]\n",
    "boston_df['pca_y'] = pca_transformed[:,1]\n",
    "\n",
    "## 군집의 개수(7개)만큼 산점도 시각화\n",
    "marker0_ind = boston_df[boston_df['cluster']==1].index\n",
    "marker1_ind = boston_df[boston_df['cluster']==1].index\n",
    "marker2_ind = boston_df[boston_df['cluster']==2].index\n",
    "marker3_ind = boston_df[boston_df['cluster']==3].index\n",
    "marker4_ind = boston_df[boston_df['cluster']==4].index\n",
    "marker5_ind = boston_df[boston_df['cluster']==5].index\n",
    "marker6_ind = boston_df[boston_df['cluster']==6].index\n",
    "plt.scatter(x=boston_df.loc[marker0_ind,'pca_x'], y=boston_df.loc[marker0_ind,'pca_y'], marker='o') \n",
    "plt.scatter(x=boston_df.loc[marker1_ind,'pca_x'], y=boston_df.loc[marker1_ind,'pca_y'], marker='s')\n",
    "plt.scatter(x=boston_df.loc[marker2_ind,'pca_x'], y=boston_df.loc[marker2_ind,'pca_y'], marker='^')\n",
    "plt.scatter(x=boston_df.loc[marker3_ind,'pca_x'], y=boston_df.loc[marker3_ind,'pca_y'], marker='v') \n",
    "plt.scatter(x=boston_df.loc[marker4_ind,'pca_x'], y=boston_df.loc[marker4_ind,'pca_y'], marker='<')\n",
    "plt.scatter(x=boston_df.loc[marker5_ind,'pca_x'], y=boston_df.loc[marker5_ind,'pca_y'], marker='>')\n",
    "plt.scatter(x=boston_df.loc[marker6_ind,'pca_x'], y=boston_df.loc[marker6_ind,'pca_y'], marker='*')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('7 Clusters Visualization by 2 PCA Components')\n",
    "plt.show()\n",
    "\n",
    "# 5) 군집별 특징 비교 & 시각화\n",
    "## 주태가격 평균 비교\n",
    "boston_df.groupby(['cluster'])['medv'].mean()\n",
    "\n",
    "## 중간값 표시를 위한 boxplot\n",
    "import seaborn as sns\n",
    "g=sns.boxplot(data=boston_df,y='medv',x='cluster')\n",
    "g.set_title('medv per Cluster')\n",
    "g.set_xlabel('Cluster')\n",
    "g.set_ylabel('medv')\n",
    "\n",
    "## 중간값이 가장 높은 / 낮은 군집의 특징 확인\n",
    "boston_df[boston_df['cluster']==4].describe()\n",
    "\n",
    "## 군집별 범죄율(crim)을 boxplot으로 시각화\n",
    "g=sns.boxplot(data=boston_df,y='crim',x='cluster')\n",
    "g.set_title('crim per Cluster')\n",
    "g.set_xlabel('Cluster')\n",
    "g.set_ylabel('crim')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클러스터링 알고리즘 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 1) 랜덤한 데이터 생성 (for 군집화 알고리즘 테스트)\n",
    "X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=0)\n",
    "  # n_samples : 생성할 총 데이터 개수\n",
    "  # n_features : 데이터의 피처 개수 (차원)\n",
    "  # centers : 군집의 개수\n",
    "  # cluster_std : 군집 내에서의 표준 편차\n",
    "unique, counts = np.unique(y, return_counts=True) ## 각 군집의 데이터 개수 확인\n",
    "print(unique, counts)\n",
    "  ## [0 1 2] [67 67 66]\n",
    "\n",
    "# 2) 데이터 프레임으로 변환\n",
    "clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])\n",
    "clusterDF['target'] = y\n",
    "\n",
    "# 3) 데이터 시각화: Scatter (군집화 전)\n",
    "target_list = np.unique(y) # 군집의 종류: [0 1 2]\n",
    "markers=['o', 's', '^', 'P','D','H','x']\n",
    "for target in target_list:\n",
    "    target_cluster = clusterDF[clusterDF['target']==target]\n",
    "    plt.scatter(x=target_cluster['ftr1'], y=target_cluster['ftr2'], edgecolor='k', marker=markers[target] )\n",
    "plt.show()\n",
    "\n",
    "# 4) KMeans 군집화\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    max_iter=200,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "## 군집화 결과(cluster_labels)를 저장\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "clusterDF['kmeans_label'] = cluster_labels\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "\n",
    "\n",
    "# 5) 군집화 결과 시각화\n",
    "## 군집 중심 좌표\n",
    "centers = kmeans.cluster_centers_\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "\n",
    "## marker\n",
    "markers=['o', 's', '^', 'P','D','H','x']\n",
    "\n",
    "## Scatter plot\n",
    "for label in unique_labels:\n",
    "    \n",
    "    # 군집 시각화\n",
    "    label_cluster = clusterDF[clusterDF['kmeans_label']==label]\n",
    "    plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], edgecolor='k', \n",
    "                marker=markers[label] )\n",
    "    \n",
    "    # 군집별 중심 위치 좌표 시각화\n",
    "    center_x_y = centers[label]\n",
    "    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color='white',\n",
    "                alpha=0.9, edgecolor='k', marker=markers[label])\n",
    "    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k', edgecolor='k', \n",
    "                marker='$%d$' % label)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가우시안 믹스처 모델 (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) 데이터 로드\n",
    "iris = load_iris()\n",
    "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "irisDF = pd.DataFrame(data=iris.data, columns=feature_names)\n",
    "irisDF['target'] = iris.target\n",
    "\n",
    "# 2) 클러스터링 결과 저장 (GMM, KMeans)\n",
    "## 2-1) GMM 클러스터링\n",
    "gmm = GaussianMixture(\n",
    "  n_components=3, # 군집 개수\n",
    "  random_state=0\n",
    ").fit(iris.data)\n",
    "irisDF['gmm_cluster'] = gmm.predict(iris.data)\n",
    "\n",
    "## 2-2) KMeans 클러스터링\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(iris.data)\n",
    "irisDF['kmeans_cluster'] = kmeans.labels_\n",
    "\n",
    "# 3) 결과 확인\n",
    "## 3-1) GMM\n",
    "print(\"\\nGMM Clustering Results:\")\n",
    "print(irisDF.groupby('target')['gmm_cluster'].value_counts())\n",
    "\n",
    "## 3-2) KMeans\n",
    "print(\"KMeans Clustering Results:\")\n",
    "print(irisDF.groupby('target')['kmeans_cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) 시각화 함수 정의\n",
    "def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):\n",
    "  if iscenter: # 중심 좌표가 있는 경우\n",
    "    centers = clusterobj.cluster_centers_\n",
    "  \n",
    "  unique_labels = np.unique(dataframe[label_name].values)\n",
    "  markers = ['o', 's', '^', 'x', '*']\n",
    "  isNoise = False\n",
    "\n",
    "  for label in unique_labels:\n",
    "    label_cluster = dataframe[dataframe[label_name] == label]\n",
    "    if label == -1: ## label이 -1인 경우, Noise로 분류\n",
    "      cluster_legend = 'Noise'\n",
    "      isNoise = True\n",
    "    else:\n",
    "      cluster_legend = 'Cluster ' + str(label) # 군집 이름\n",
    "    \n",
    "    # 군집별 시각화\n",
    "    plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,\n",
    "          edgecolor='k', marker=markers[label], label=cluster_legend)\n",
    "    \n",
    "    # 군집 중심점 시각화\n",
    "    if iscenter:\n",
    "      center_x_y = centers[label]\n",
    "      plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',\n",
    "            alpha=0.9, edgecolor='k', marker=markers[label])\n",
    "      plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',\n",
    "            edgecolor='k', marker='$%d$' % label)\n",
    "  if isNoise:\n",
    "    legend_loc = 'upper center' # Noise의 경우, legend(범례) 위치를 달리하여 표시\n",
    "  else:\n",
    "    legend_loc = 'upper right' # Noise가 아닌 경우, 기본 위치\n",
    "  \n",
    "  plt.legend(loc=legend_loc)\n",
    "  plt.show()\n",
    "\n",
    "# 1) 데이터 생성\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.5, random_state=0)\n",
    "clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])\n",
    "clusterDF['target'] = y\n",
    "\n",
    "# 2) 데이터 시각화 (탐색)\n",
    "visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)\n",
    "\n",
    "## 타원형 데이터셋으로 변환\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation) # Linear Transformation\n",
    "clusterDF = pd.DataFrame(data=X_aniso, columns=['ftr1', 'ftr2'])\n",
    "clusterDF['target'] = y\n",
    "visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)\n",
    "\n",
    "# 3) GMM, KMeans 클러스터링 결과 비교\n",
    "## 3-1) GMM 클러스터링 시각화\n",
    "gmm = GaussianMixture(n_components=3, random_state=0)\n",
    "clusterDF['gmm_label'] = gmm.fit(X_aniso).predict(X_aniso)\n",
    "\n",
    "print('\\n### Gaussian Mixture Clustering ###')\n",
    "print(clusterDF.groupby('target')['gmm_label'].value_counts())\n",
    "visualize_cluster_plot(gmm, clusterDF, 'gmm_label', iscenter=False)\n",
    "\n",
    "## 3-2) KMeans 클러스터링 시각화\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans_label = kmeans.fit_predict(X_aniso)\n",
    "clusterDF['kmeans_label'] = kmeans_label\n",
    "  ## 또는 clusterDF['kmeans_label'] = kmeans.labels_\n",
    "\n",
    "print('### KMeans Clustering ###')\n",
    "print(clusterDF.groupby('target')['kmeans_label'].value_counts())\n",
    "visualize_cluster_plot(kmeans, clusterDF, 'kmeans_label', iscenter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드\n",
    "boston_df = pd.read_csv('../../data/boston_housing.csv')\n",
    "boston_df\n",
    "\n",
    "# 2) 데이터 전처리\n",
    "## 2-1) Min-Max 스케일링\n",
    "boston_scaled = MinMaxScaler().fit_transform(boston_df.drop('medv', axis=1))\n",
    "boston_scaled_df=pd.DataFrame(data=boston_scaled, columns=boston_df.columns[:-1])\n",
    "\n",
    "## 2-2) PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_transformed = pca.fit_transform(boston_scaled_df)\n",
    "\n",
    "boston_df['ftr1'] = pca_transformed[:,0]\n",
    "boston_df['ftr2'] = pca_transformed[:,1]\n",
    "\n",
    "# 3) GMM 군집화\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=7, random_state=0).fit(boston_scaled_df)\n",
    "boston_df['cluster']=gmm.predict(boston_scaled_df)\n",
    "\n",
    "# 4) 군집별 시각화\n",
    "visualize_cluster_plot(gmm, boston_df,'cluster',iscenter=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN 군집화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 데이터 로드\n",
    "iris = load_iris()\n",
    "feature_names = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "irisDF = pd.DataFrame(data=iris.data, columns=feature_names)\n",
    "irisDF['target'] = iris.target\n",
    "\n",
    "# 2) DBSCAN 군집화 수행 및 결과 저장\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(\n",
    "  eps=0.6, # 같은 군집으로 인식할 데이터포인트의 최대 거리\n",
    "  min_samples=8, # 군집을 형성하기 위한 최소 데이터포인트 수\n",
    "  metric='euclidean' # 거리 측정 방식\n",
    ")\n",
    "dbscan_labels = dbscan.fit_predict(iris.data)\n",
    "irisDF['dbscan_cluster'] = dbscan_labels\n",
    "irisDF['target'] = iris.target\n",
    "\n",
    "# 3) 군집화 결과 확인\n",
    "iris_result = irisDF.groupby(['target'])['dbscan_cluster'].value_counts()\n",
    "print(iris_result)\n",
    "\n",
    "# 4) 군집 시각화\n",
    "## 4-1) PCA 차원 축소\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pca_transformed = pca.fit_transform(iris.data)\n",
    "irisDF['ftr1'] = pca_transformed[:,0]\n",
    "irisDF['ftr2'] = pca_transformed[:,1]\n",
    "\n",
    "## 4-2) 군집 시각화\n",
    "visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 로드 (make_circle)\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, shuffle=True, noise=0.05, random_state=0, factor=0.5)\n",
    "clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])\n",
    "clusterDF['target'] = y\n",
    "\n",
    "# 2) 데이터 시각화 (탐색)\n",
    "visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)\n",
    "\n",
    "# 3) Kmeans / GMM / MeanShift / DBSCAN 클러스터링 비교 (시각화)\n",
    "## 3-1) Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, max_iter=1000, random_state=0)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "clusterDF['kmeans_cluster'] = kmeans_labels\n",
    "\n",
    "visualize_cluster_plot(kmeans, clusterDF, 'kmeans_cluster', iscenter=True)\n",
    "\n",
    "## 3-2) GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=2, random_state=0)\n",
    "gmm_label = gmm.fit(X).predict(X)\n",
    "clusterDF['gmm_cluster'] = gmm_label\n",
    "\n",
    "visualize_cluster_plot(gmm, clusterDF, 'gmm_cluster', iscenter=False)\n",
    "\n",
    "## 3-3) MeanShift\n",
    "from sklearn.cluster import MeanShift\n",
    "meanshift= MeanShift(\n",
    "  bandwidth=0.9 # 커널의 크기 (-> 클러스터의 크기에 영향을 미침)\n",
    ")\n",
    "meanshift_label = meanshift.fit_predict(X)\n",
    "clusterDF['ms_cluster'] = meanshift_label\n",
    "\n",
    "visualize_cluster_plot(meanshift, clusterDF, 'ms_cluster', iscenter=False)\n",
    "\n",
    "## 3-4) DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=10, metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "clusterDF['dbscan_cluster'] = dbscan_labels\n",
    "\n",
    "visualize_cluster_plot(dbscan, clusterDF, 'dbscan_cluster', iscenter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) 데이터 로드\n",
    "boston_df = pd.read_csv('boston.csv')\n",
    "\n",
    "# 2) 데이터 전처리\n",
    "## 2-1) Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "boston_scaled = scaler.fit_transform(boston_df.drop('medv', axis=1))\n",
    "boston_scaled_df = pd.DataFrame(boston_scaled, columns=boston_df.columns[:-1])\n",
    "\n",
    "## 2-2) PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_transformed = pca.fit_transform(boston_scaled_df)\n",
    "boston_df['ftr1'] = pca_transformed[:,0]\n",
    "boston_df['ftr2'] = pca_transformed[:,1]\n",
    "\n",
    "# 3) DBSCAN 군집화 & 시각화\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(boston_scaled_df)\n",
    "boston_df['dbscan_cluster_0_5_5'] = dbscan_labels\n",
    "visualize_cluster_plot(dbscan, boston_df, 'dbscan_cluster_0_5_5', iscenter=False)\n",
    "\n",
    "## 이후에 eps, min_samples를 조정하여 군집화 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연관 규칙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 1) 데이터 로드 (트랜잭션 데이터)\n",
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "       ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "       ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "       ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "       ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "\n",
    "# 2) 트랜잭션 인코딩\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset) ## 이진(Binary) 인코딩\n",
    "       ## array([[False False  True False  True  True  True  True  True], [...]])\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# 3) ItemSet 생성\n",
    "frequent_itemsets = apriori( ## apriori: 빈번한 아이템 집합을 찾는 알고리즘\n",
    "       df, # 인코딩된 데이터\n",
    "       min_support=0.6, # 최소 지지도\n",
    "       use_colnames=True # 아이템명 사용\n",
    ")\n",
    "\n",
    "# 4) 연관 규칙 생성\n",
    "rules = association_rules(\n",
    "       frequent_itemsets, # ItemSet\n",
    "       metric=\"lift\", # 규칙 평가 척도 (lift || confidence)\n",
    "       min_threshold=1.2 # 기준(metric)의 최솟값\n",
    ")\n",
    "#      antecedents(조건)  consequents(결과)  support  confidence   lift\n",
    "# 0         (Milk)        (Kidney Beans)    0.6        0.75    1.25\n",
    "# 1         (Eggs)        (Kidney Beans)    0.8        0.89    1.35\n",
    "\n",
    "\n",
    "# 5) 연관규칙 결과 조회\n",
    "rules\n",
    "## 필터링 (조건에 맞는 연관규칙 조회)\n",
    "rules[ (rules['antecedent_len'] >= 2) &\n",
    "       (rules['confidence'] > 0.75) &\n",
    "       (rules['lift'] > 1.2) ]\n",
    "rules[rules['antecedents'] == {'Eggs', 'Kidney Beans'}]\n",
    "rules[rules['antecedents'].apply(lambda x: 'Eggs' in x)]\n",
    "\n",
    "rules.iloc[rules['lift'].idxmax()] # lift가 가장 큰 규칙\n",
    "\n",
    "# Top 10 규칙\n",
    "rules.sort_values('confidence',ascending=False)[['antecedents','consequents','support','confidence','lift']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm-3-12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
